[2025-12-08T07:51:07.115+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-08T07:51:07.157+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mongo_iceberg_dremio_dag.run_mongo_to_iceberg manual__2025-12-08T07:51:05.123855+00:00 [queued]>
[2025-12-08T07:51:07.167+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mongo_iceberg_dremio_dag.run_mongo_to_iceberg manual__2025-12-08T07:51:05.123855+00:00 [queued]>
[2025-12-08T07:51:07.168+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2025-12-08T07:51:07.266+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): run_mongo_to_iceberg> on 2025-12-08 07:51:05.123855+00:00
[2025-12-08T07:51:07.283+0000] {standard_task_runner.py:63} INFO - Started process 255 to run task
[2025-12-08T07:51:07.298+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mongo_iceberg_dremio_dag', 'run_mongo_to_iceberg', 'manual__2025-12-08T07:51:05.123855+00:00', '--job-id', '75', '--raw', '--subdir', 'DAGS_FOLDER/spark_read_mongo_dag.py', '--cfg-path', '/tmp/tmpiyuo78lc']
[2025-12-08T07:51:07.313+0000] {standard_task_runner.py:91} INFO - Job 75: Subtask run_mongo_to_iceberg
[2025-12-08T07:51:07.708+0000] {task_command.py:426} INFO - Running <TaskInstance: mongo_iceberg_dremio_dag.run_mongo_to_iceberg manual__2025-12-08T07:51:05.123855+00:00 [running]> on host ff9722fe1231
[2025-12-08T07:51:07.923+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='deeku' AIRFLOW_CTX_DAG_ID='mongo_iceberg_dremio_dag' AIRFLOW_CTX_TASK_ID='run_mongo_to_iceberg' AIRFLOW_CTX_EXECUTION_DATE='2025-12-08T07:51:05.123855+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-12-08T07:51:05.123855+00:00'
[2025-12-08T07:51:07.924+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-08T07:51:07.949+0000] {spark_read_mongo_dag.py:151} INFO - START run_spark_job
[2025-12-08T07:51:07.951+0000] {spark_read_mongo_dag.py:155} INFO - Checking path: /opt/***/spark_jobs/mongo_pyspark.py exists=True
[2025-12-08T07:51:07.953+0000] {spark_read_mongo_dag.py:155} INFO - Checking path: /opt/***/jars exists=True
[2025-12-08T07:51:07.955+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/mongo-spark-connector_2.12-10.1.1.jar exists=True size=156368
[2025-12-08T07:51:07.957+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/bson-4.11.2.jar exists=True size=511356
[2025-12-08T07:51:07.959+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/mongodb-driver-core-4.11.2.jar exists=True size=1670650
[2025-12-08T07:51:07.961+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/mongodb-driver-sync-4.11.2.jar exists=True size=151808
[2025-12-08T07:51:07.963+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar exists=True size=41604737
[2025-12-08T07:51:07.965+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/iceberg-nessie-1.5.2.jar exists=True size=44895
[2025-12-08T07:51:07.967+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/nessie-client-0.99.0.jar exists=True size=585165
[2025-12-08T07:51:07.969+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/nessie-spark-extensions-3.4_2.12-0.105.7.jar exists=True size=2294495
[2025-12-08T07:51:07.972+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/hadoop-aws-3.3.4.jar exists=True size=962685
[2025-12-08T07:51:07.976+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/aws-java-sdk-bundle-1.12.772.jar exists=True size=388405806
[2025-12-08T07:51:07.977+0000] {spark_read_mongo_dag.py:182} INFO - Running command: spark-submit --jars /opt/***/jars/mongo-spark-connector_2.12-10.1.1.jar,/opt/***/jars/bson-4.11.2.jar,/opt/***/jars/mongodb-driver-core-4.11.2.jar,/opt/***/jars/mongodb-driver-sync-4.11.2.jar,/opt/***/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar,/opt/***/jars/iceberg-nessie-1.5.2.jar,/opt/***/jars/nessie-client-0.99.0.jar,/opt/***/jars/nessie-spark-extensions-3.4_2.12-0.105.7.jar,/opt/***/jars/hadoop-aws-3.3.4.jar,/opt/***/jars/aws-java-sdk-bundle-1.12.772.jar /opt/***/spark_jobs/mongo_pyspark.py
[2025-12-08T07:51:07.978+0000] {spark_read_mongo_dag.py:183} INFO - ICEBERG_WAREHOUSE=s3a://iceberg
[2025-12-08T07:51:07.978+0000] {spark_read_mongo_dag.py:184} INFO - NESSIE_URI=http://172.19.0.6:19120
[2025-12-08T07:51:07.979+0000] {spark_read_mongo_dag.py:185} INFO - S3_ENDPOINT=http://minio:9000
[2025-12-08T07:52:15.414+0000] {spark_read_mongo_dag.py:189} INFO - Process finished. returncode=0
[2025-12-08T07:52:15.415+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:51:59,292 INFO mongo_to_iceberg - spark.sql.catalog.nessie.warehouse=s3a://iceberg
[2025-12-08T07:52:15.416+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:51:59,292 INFO mongo_to_iceberg - Starting read from MongoDB collection ai_segmentation.conversations
[2025-12-08T07:52:15.416+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:02,844 INFO mongo_to_iceberg - Read succeeded. Counting rows (this may be slow)...
[2025-12-08T07:52:15.416+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,186 WARNING mongo_to_iceberg - Failed to count rows; assuming 0. Error: URI doesn't end with the version: http://172.19.0.6:19120. Please configure `client-api-version` in the catalog properties explicitly.
[2025-12-08T07:52:15.417+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,186 INFO mongo_to_iceberg - Rows read from MongoDB: 0
[2025-12-08T07:52:15.417+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,186 INFO mongo_to_iceberg - Showing up to 5 rows:
[2025-12-08T07:52:15.417+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,273 WARNING mongo_to_iceberg - Failed to take sample rows: URI doesn't end with the version: http://172.19.0.6:19120. Please configure `client-api-version` in the catalog properties explicitly.
[2025-12-08T07:52:15.418+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,523 ERROR mongo_to_iceberg - Failed to print JSON rows: URI doesn't end with the version: http://172.19.0.6:19120. Please configure `client-api-version` in the catalog properties explicitly.
[2025-12-08T07:52:15.418+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,523 INFO mongo_to_iceberg - Namespace: ai_segmentation, Table: conversations, Table ident: nessie.ai_segmentation.conversations
[2025-12-08T07:52:15.418+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,523 INFO mongo_to_iceberg - Namespace location: s3a://iceberg/ai_segmentation
[2025-12-08T07:52:15.418+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,523 INFO mongo_to_iceberg - Parquet target path (for fallback): s3a://iceberg/ai_segmentation/conversations/parquet/
[2025-12-08T07:52:15.419+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:06,523 WARNING mongo_to_iceberg - DataFrame appears empty or has no columns (rows=0). Will write a job marker instead.
[2025-12-08T07:52:15.419+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:13,825 WARNING mongo_to_iceberg - Failed to write job marker: URI doesn't end with the version: http://172.19.0.6:19120. Please configure `client-api-version` in the catalog properties explicitly.
[2025-12-08T07:52:15.419+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:13,831 INFO mongo_to_iceberg - Writing CSV preview (limit=100) to s3a://iceberg/ai_segmentation/conversations/preview/
[2025-12-08T07:52:15.420+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:13,943 WARNING mongo_to_iceberg - Failed to write CSV preview: URI doesn't end with the version: http://172.19.0.6:19120. Please configure `client-api-version` in the catalog properties explicitly.
[2025-12-08T07:52:15.420+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:13,943 INFO mongo_to_iceberg - Verifying Iceberg table read via Spark SQL: SELECT count(*) FROM nessie.ai_segmentation.conversations
[2025-12-08T07:52:15.420+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:14,055 WARNING mongo_to_iceberg - Failed to verify Iceberg table via SQL: URI doesn't end with the version: http://172.19.0.6:19120. Please configure `client-api-version` in the catalog properties explicitly.
[2025-12-08T07:52:15.420+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:14,055 INFO mongo_to_iceberg - Stopping Spark.
[2025-12-08T07:52:15.421+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:14,729 INFO mongo_to_iceberg - END main SUCCESS at 2025-12-08 07:52:14
[2025-12-08T07:52:15.421+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 07:52:14,730 INFO py4j.clientserver - Closing down clientserver connection
[2025-12-08T07:52:15.421+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-12-08T07:52:15.422+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-12-08T07:52:15.422+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-12-08T07:52:15.422+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-12-08T07:52:15.422+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | at java.base/java.lang.Thread.run(Thread.java:840)
[2025-12-08T07:52:15.423+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-12-08T07:52:15.423+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO SparkUI: Stopped Spark web UI at http://ff9722fe1231:4040
[2025-12-08T07:52:15.423+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-12-08T07:52:15.423+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO MemoryStore: MemoryStore cleared
[2025-12-08T07:52:15.424+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO BlockManager: BlockManager stopped
[2025-12-08T07:52:15.424+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-12-08T07:52:15.424+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-12-08T07:52:15.424+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:14 INFO SparkContext: Successfully stopped SparkContext
[2025-12-08T07:52:15.425+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:15 INFO ShutdownHookManager: Shutdown hook called
[2025-12-08T07:52:15.425+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-22a8e739-849a-4e77-b5a2-86e40d6704bc
[2025-12-08T07:52:15.425+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-beaa602f-5481-4236-82e5-c0d741d58994
[2025-12-08T07:52:15.426+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-beaa602f-5481-4236-82e5-c0d741d58994/pyspark-18cb98a8-cf15-440d-ae8d-0fb777bb1bcb
[2025-12-08T07:52:15.426+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:15 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-12-08T07:52:15.427+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:15 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-12-08T07:52:15.428+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 07:52:15 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-12-08T07:52:15.428+0000] {spark_read_mongo_dag.py:206} INFO - END run_spark_job SUCCESS
[2025-12-08T07:52:15.428+0000] {python.py:237} INFO - Done. Returned value was: {'status': 'success', 'message': 'Spark job completed'}
[2025-12-08T07:52:15.429+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-08T07:52:15.663+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=mongo_iceberg_dremio_dag, task_id=run_mongo_to_iceberg, run_id=manual__2025-12-08T07:51:05.123855+00:00, execution_date=20251208T075105, start_date=20251208T075107, end_date=20251208T075215
[2025-12-08T07:52:15.960+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-12-08T07:52:16.340+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-12-08T07:52:16.387+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
