[2025-12-08T09:30:40.154+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-12-08T09:30:40.191+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mongo_iceberg_dremio_dag.run_mongo_to_iceberg manual__2025-12-08T09:30:39.210628+00:00 [queued]>
[2025-12-08T09:30:40.200+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mongo_iceberg_dremio_dag.run_mongo_to_iceberg manual__2025-12-08T09:30:39.210628+00:00 [queued]>
[2025-12-08T09:30:40.202+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2025-12-08T09:30:40.261+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): run_mongo_to_iceberg> on 2025-12-08 09:30:39.210628+00:00
[2025-12-08T09:30:40.266+0000] {standard_task_runner.py:63} INFO - Started process 2606 to run task
[2025-12-08T09:30:40.270+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'mongo_iceberg_dremio_dag', 'run_mongo_to_iceberg', 'manual__2025-12-08T09:30:39.210628+00:00', '--job-id', '85', '--raw', '--subdir', 'DAGS_FOLDER/spark_read_mongo_dag.py', '--cfg-path', '/tmp/tmpvec9j2fv']
[2025-12-08T09:30:40.273+0000] {standard_task_runner.py:91} INFO - Job 85: Subtask run_mongo_to_iceberg
[2025-12-08T09:30:40.408+0000] {task_command.py:426} INFO - Running <TaskInstance: mongo_iceberg_dremio_dag.run_mongo_to_iceberg manual__2025-12-08T09:30:39.210628+00:00 [running]> on host ef1c6bdacaab
[2025-12-08T09:30:40.661+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='deeku' AIRFLOW_CTX_DAG_ID='mongo_iceberg_dremio_dag' AIRFLOW_CTX_TASK_ID='run_mongo_to_iceberg' AIRFLOW_CTX_EXECUTION_DATE='2025-12-08T09:30:39.210628+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-12-08T09:30:39.210628+00:00'
[2025-12-08T09:30:40.662+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-12-08T09:30:40.674+0000] {spark_read_mongo_dag.py:151} INFO - START run_spark_job
[2025-12-08T09:30:40.675+0000] {spark_read_mongo_dag.py:155} INFO - Checking path: /opt/***/spark_jobs/mongo_pyspark.py exists=True
[2025-12-08T09:30:40.676+0000] {spark_read_mongo_dag.py:155} INFO - Checking path: /opt/***/jars exists=True
[2025-12-08T09:30:40.679+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/mongo-spark-connector_2.12-10.1.1.jar exists=True size=156368
[2025-12-08T09:30:40.681+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/bson-4.11.2.jar exists=True size=511356
[2025-12-08T09:30:40.682+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/mongodb-driver-core-4.11.2.jar exists=True size=1670650
[2025-12-08T09:30:40.684+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/mongodb-driver-sync-4.11.2.jar exists=True size=151808
[2025-12-08T09:30:40.685+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar exists=True size=41604737
[2025-12-08T09:30:40.686+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/iceberg-nessie-1.5.2.jar exists=True size=44895
[2025-12-08T09:30:40.687+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/nessie-client-0.99.0.jar exists=True size=585165
[2025-12-08T09:30:40.688+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/nessie-spark-extensions-3.4_2.12-0.105.7.jar exists=True size=2294495
[2025-12-08T09:30:40.690+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/hadoop-aws-3.3.4.jar exists=True size=962685
[2025-12-08T09:30:40.691+0000] {spark_read_mongo_dag.py:161} INFO - Jar check: /opt/***/jars/aws-java-sdk-bundle-1.12.772.jar exists=True size=388405806
[2025-12-08T09:30:40.691+0000] {spark_read_mongo_dag.py:182} INFO - Running command: spark-submit --jars /opt/***/jars/mongo-spark-connector_2.12-10.1.1.jar,/opt/***/jars/bson-4.11.2.jar,/opt/***/jars/mongodb-driver-core-4.11.2.jar,/opt/***/jars/mongodb-driver-sync-4.11.2.jar,/opt/***/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar,/opt/***/jars/iceberg-nessie-1.5.2.jar,/opt/***/jars/nessie-client-0.99.0.jar,/opt/***/jars/nessie-spark-extensions-3.4_2.12-0.105.7.jar,/opt/***/jars/hadoop-aws-3.3.4.jar,/opt/***/jars/aws-java-sdk-bundle-1.12.772.jar /opt/***/spark_jobs/mongo_pyspark.py
[2025-12-08T09:30:40.692+0000] {spark_read_mongo_dag.py:183} INFO - ICEBERG_WAREHOUSE=s3a://iceberg
[2025-12-08T09:30:40.692+0000] {spark_read_mongo_dag.py:184} INFO - NESSIE_URI=http://nessie:19120/api/v1
[2025-12-08T09:30:40.692+0000] {spark_read_mongo_dag.py:185} INFO - S3_ENDPOINT=http://minio:9000
[2025-12-08T09:31:19.174+0000] {spark_read_mongo_dag.py:189} INFO - Process finished. returncode=0
[2025-12-08T09:31:19.175+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,783 INFO mongo_to_iceberg - {"_id":"019ac3ad-9782-7225-a121-b051e89efc1f","messages":[{"id":"msg_a42b25da","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-27T04:59:04.968000"},{"id":"msg_00dc5fe3","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-27T04:59:04.981000"}],"updatedAt":"2025-11-27T04:59:04.981000"}
[2025-12-08T09:31:19.176+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,783 INFO mongo_to_iceberg - {"_id":"019ac3b9-da3c-7774-b7dc-182fe4fd2c84","messages":[{"id":"msg_563d31b2","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-27T05:12:18.151000"},{"id":"msg_756eae78","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-27T05:12:18.163000"}],"updatedAt":"2025-11-27T05:12:18.163000"}
[2025-12-08T09:31:19.176+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,783 INFO mongo_to_iceberg - {"_id":"019ac3b2-039b-755b-8b3c-80968af1e395","messages":[{"id":"msg_cf8eec23","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-27T05:03:55.676000"},{"id":"msg_61b15fa3","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-27T05:03:55.705000"},{"id":"msg_8a86b8d9","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-27T05:04:42.990000"},{"id":"msg_d0c54fd4","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-27T05:04:42.997000"}],"updatedAt":"2025-11-27T05:04:42.997000"}
[2025-12-08T09:31:19.177+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,783 INFO mongo_to_iceberg - {"_id":"019ac3ba-d1c0-7112-b32c-c1d20960a58f","messages":[{"id":"msg_fce772e1","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-27T05:13:18.138000"},{"id":"msg_12104ee3","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-27T05:13:18.145000"}],"updatedAt":"2025-11-27T05:13:18.145000"}
[2025-12-08T09:31:19.177+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,783 INFO mongo_to_iceberg - {"_id":"019ac3bb-67ae-7331-9028-e567e2e01b50","messages":[{"id":"msg_729af5e0","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-27T05:13:55.692000"},{"id":"msg_e647c561","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-27T05:13:55.714000"}],"updatedAt":"2025-11-27T05:13:55.714000"}
[2025-12-08T09:31:19.178+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,784 INFO mongo_to_iceberg - {"_id":"019ac3c4-3f6b-7445-8f4d-a1da264d5d47","messages":[{"id":"msg_9a05c709","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-27T05:23:36.481000"},{"id":"msg_b786951c","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-27T05:23:36.488000"}],"updatedAt":"2025-11-27T05:23:36.488000"}
[2025-12-08T09:31:19.179+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,784 INFO mongo_to_iceberg - {"_id":"019ab99c-d73e-7009-939f-dd453cd76b1f","messages":[{"id":"msg_9f471db9","text":"Show me all users who visited the site in the last 7 days","type":"user","timestamp":"2025-11-25T06:04:55.473000"},{"id":"msg_72401095","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-25T06:04:55.480000"},{"id":"msg_70f74847","text":"Show me all users who site visited in the last 3 days","type":"user","timestamp":"2025-11-25T06:05:17.223000"},{"id":"msg_af986bb4","text":"Segentation processed successfully","type":"assistant","timestamp":"2025-11-25T06:05:17.234000"}],"updatedAt":"2025-11-25T06:05:17.234000"}
[2025-12-08T09:31:19.179+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,784 INFO mongo_to_iceberg - Namespace: ai_segmentation, Table: conversations, Table ident: nessie.ai_segmentation.conversations
[2025-12-08T09:31:19.180+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,784 INFO mongo_to_iceberg - Namespace location: s3a://iceberg/ai_segmentation
[2025-12-08T09:31:19.180+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,784 INFO mongo_to_iceberg - Parquet target path (for fallback): s3a://iceberg/ai_segmentation/conversations/parquet/
[2025-12-08T09:31:19.180+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:05,786 INFO mongo_to_iceberg - Ensuring Iceberg namespace exists: nessie.ai_segmentation
[2025-12-08T09:31:19.181+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:06,458 INFO mongo_to_iceberg - Writing DataFrame as Iceberg table via Nessie catalog: nessie.ai_segmentation.conversations
[2025-12-08T09:31:19.181+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:13,446 INFO mongo_to_iceberg - Iceberg write successful for table: nessie.ai_segmentation.conversations
[2025-12-08T09:31:19.182+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:13,449 INFO mongo_to_iceberg - Writing CSV preview (limit=100) to s3a://iceberg/ai_segmentation/conversations/preview/
[2025-12-08T09:31:19.182+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:13,819 WARNING mongo_to_iceberg - Failed to write CSV preview: Column `messages` has a data type of array<struct<id:string,text:string,type:string,timestamp:string>>, which is not supported by CSV.
[2025-12-08T09:31:19.183+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:13,819 INFO mongo_to_iceberg - Verifying Iceberg table read via Spark SQL: SELECT count(*) FROM nessie.ai_segmentation.conversations
[2025-12-08T09:31:19.183+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:14,426 INFO mongo_to_iceberg - Verification count result: [Row(cnt=23)]
[2025-12-08T09:31:19.183+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:14,427 INFO mongo_to_iceberg - Stopping Spark.
[2025-12-08T09:31:19.184+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:14,755 INFO mongo_to_iceberg - END main SUCCESS at 2025-12-08 09:31:14
[2025-12-08T09:31:19.184+0000] {spark_read_mongo_dag.py:195} INFO - spark-submit stdout | 2025-12-08 09:31:14,756 INFO py4j.clientserver - Closing down clientserver connection
[2025-12-08T09:31:19.185+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[2025-12-08T09:31:19.185+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/spark_jobs/mongo_pyspark.py:269) finished in 0.083 s
[2025-12-08T09:31:19.186+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-08T09:31:19.186+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-12-08T09:31:19.186+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO DAGScheduler: Job 5 finished: collect at /opt/***/spark_jobs/mongo_pyspark.py:269, took 0.091816 s
[2025-12-08T09:31:19.187+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-12-08T09:31:19.187+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO SparkUI: Stopped Spark web UI at http://ef1c6bdacaab:4040
[2025-12-08T09:31:19.187+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-12-08T09:31:19.188+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO MemoryStore: MemoryStore cleared
[2025-12-08T09:31:19.188+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO BlockManager: BlockManager stopped
[2025-12-08T09:31:19.188+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-12-08T09:31:19.189+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-12-08T09:31:19.189+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:14 INFO SparkContext: Successfully stopped SparkContext
[2025-12-08T09:31:19.189+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:18 INFO ShutdownHookManager: Shutdown hook called
[2025-12-08T09:31:19.190+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb693107-c0ab-423a-9538-c7b64d3b1af6
[2025-12-08T09:31:19.190+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-27a839eb-a4d7-4ae1-9f5f-638ef2b6a25a/pyspark-b619620b-db06-454c-a4bc-a7d2cff42ef2
[2025-12-08T09:31:19.190+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-27a839eb-a4d7-4ae1-9f5f-638ef2b6a25a
[2025-12-08T09:31:19.191+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:18 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-12-08T09:31:19.191+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:18 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-12-08T09:31:19.191+0000] {spark_read_mongo_dag.py:201} ERROR - spark-submit stderr | 25/12/08 09:31:18 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-12-08T09:31:19.192+0000] {spark_read_mongo_dag.py:206} INFO - END run_spark_job SUCCESS
[2025-12-08T09:31:19.192+0000] {python.py:237} INFO - Done. Returned value was: {'status': 'success', 'message': 'Spark job completed'}
[2025-12-08T09:31:19.193+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-12-08T09:31:19.284+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=mongo_iceberg_dremio_dag, task_id=run_mongo_to_iceberg, run_id=manual__2025-12-08T09:30:39.210628+00:00, execution_date=20251208T093039, start_date=20251208T093040, end_date=20251208T093119
[2025-12-08T09:31:19.381+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-12-08T09:31:19.411+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-12-08T09:31:19.464+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
